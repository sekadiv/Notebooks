{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python NNs venv",
      "language": "python",
      "name": "rnn_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Building+a+CNN+MNIST.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8trGbYycyX"
      },
      "source": [
        "# Building a Basic CNN: The MNIST Dataset\n",
        "\n",
        "In this notebook, we will build a simple CNN-based architecture to classify the 10 digits (0-9) of the MNIST dataset. The objective of this notebook is to become familiar with the process of building CNNs in TensorFlow.\n",
        "\n",
        "We will go through the following steps:\n",
        "1. Importing libraries and the dataset\n",
        "2. Data preparation: Train-test split, specifying the shape of the input data etc.\n",
        "3. Building and understanding the CNN architecture \n",
        "4. Fitting and evaluating the model\n",
        "\n",
        "Let's dive in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4i2gfPsycya"
      },
      "source": [
        "## 1. Importing Libraries and the Dataset\n",
        "\n",
        "Let's load the required libraries. From Keras, we need to import two main components:\n",
        "1. `Sequential` from `tensrflow.keras.models`: `Sequential` is the keras abstraction for creating models with a stack of layers (MLP has multiple hidden layers, CNNs have convolutional layers, etc.). \n",
        "2. Various types of layers from `tensrflow.keras.layers`: These layers are added (one after the other) to the `Sequential` model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6OM8KVoycye"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# importing the required libraries\n",
        "import tensorflow as tf\n",
        "\n",
        "# importing different elements (datasets, layers, etc.) to build the CNN model\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D , Dropout, MaxPooling2D\n",
        "\n",
        "# Ignoring the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h12FfQasycyf"
      },
      "source": [
        "Let's load the MNIST dataset from `tensrflow.keras.datasets`. The download may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-bJeejrycyf"
      },
      "source": [
        "# load the dataset into train and test sets\n",
        "mnist = tf.keras.datasets.mnist\n",
        " \n",
        "(train_images,train_labels),(test_images, test_labels)=mnist.load_data()\n",
        "\n",
        "# Defining the class names\n",
        "class_names = ['0','1','2','3','4','5','6','7','8','9']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqutCozKycyg",
        "outputId": "3d5141a9-a757-451c-c972-f8da7343cd1f"
      },
      "source": [
        "# Explore the training data as follows:\n",
        "print(train_images.shape)\n",
        "print(len(train_labels))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hTlXPlAcuJQ",
        "outputId": "9b817dd7-64de-4cb9-9e8b-3696ae878606"
      },
      "source": [
        "# Explore the test data as follows:\n",
        "print(test_images.shape)\n",
        "print(len(test_labels))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 28, 28)\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QIZTkbuycyi"
      },
      "source": [
        "So we have 60,000 training and 10,000 test images each of size 28 x 28. Note that the images are grayscale and thus are stored as 2D arrays.<br> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVOw5KjWdWiz"
      },
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Let's prepare the dataset for feeding to the network. We will do the following three main steps:<br>\n",
        "\n",
        "#### 2.1 Reshape the Data\n",
        "First, let's understand the shape in which the network expects the training data. \n",
        "Since we have 20,000 training samples each of size (28, 28, 1), the training data (`train_images`) needs to be of the shape `(20000, 28, 28, 1)`. If the images were coloured, the shape would have been `(20000, 28, 28, 3)`.\n",
        "\n",
        "Same goes for the `test_images`.\n",
        "\n",
        "#### 2.2 Rescaling (Normalisation)\n",
        "The value of each pixel is between 0-255, so we will **rescale each pixel** by dividing by 255 so that the range becomes 0-1. Recollect <a href=\"https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn\">why normalisation is important for training NNs</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VukH4iXhlGWv",
        "outputId": "f2aa50fc-af24-4250-863e-39928cc892d1"
      },
      "source": [
        "# reshape x_train and x_test\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN3_HqocmCuu"
      },
      "source": [
        "# normalisation\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnZcGhNDycyq"
      },
      "source": [
        "## 3. Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LcbjBCaycyq"
      },
      "source": [
        "Let's now build the CNN architecture. For the MNIST dataset, we do not need to build a very sophisticated CNN - a simple shallow-ish CNN would suffice. \n",
        "\n",
        "We will build a network with:\n",
        "- two convolutional layers having 32 and 64 filters respectively, \n",
        "- followed by a max pooling layer, \n",
        "- and then `Flatten` the output of the pooling layer to give us a long vector, \n",
        "- then add a fully connected `Dense` layer with 128 neurons, and finally\n",
        "- add a `softmax` layer with 10 neurons\n",
        "\n",
        "The generic way to build a model in Keras is to instantiate a `Sequential` model and keep adding `tensorflow.keras.layers` to it. We will also use some dropouts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "XX5bykTdycyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ef6429-8519-42fa-b43e-e0ee3f145c73"
      },
      "source": [
        "# model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolution and Pooling layers\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flattening and Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Adding the output layer\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri3W44yaycyr"
      },
      "source": [
        "#### Understanding Model Summary\n",
        "\n",
        "It is a good practice to spend some time staring at the model summary above and verify the number of parameteres, output sizes etc. Let's do some calculations to verify that we understand the model deeply enough. \n",
        "\n",
        "- Layer-1 (Conv2D): We have used 32 kernels of size (3, 3), and each kernel has a single bias, so we have 32 x 3 x 3 (weights) + 32 (biases) = 320 parameters (all trainable). Note that the kernels have only one channel since the input images are 2D (grayscale). By default, a convolutional layer uses stride of 1 and no padding, so the output from this layer is of shape 26 x 26 x 32, as shown in the summary above (the first element `None` is for the batch size).\n",
        "\n",
        "- Layer-2 (Conv2D): We have used 64 kernels of size (3, 3), but this time, each kernel has to convolve a tensor of size (26, 26, 32) from the previous layer. Thus, the kernels will also have 32 channels, and so the shape of each kernel is (3, 3, 32) (and we have 64 of them). So we have 64 x 3 x 3 x 32 (weights) + 64 (biases) = 18496 parameters (all trainable). The output shape is (24, 24, 64) since each kernel produces a (24, 24) feature map.\n",
        "\n",
        "- Max pooling: The pooling layer gets the (24, 24, 64) input from the previous conv layer and produces a (12, 12, 64) output (the default pooling uses stride of 2). There are no trainable parameters in the pooling layer.\n",
        "\n",
        "- The `Dropout` layer does not alter the output shape and has no trainable parameters.\n",
        "\n",
        "- The `Flatten` layer simply takes in the (12, 12, 64) output from the previous layer and 'flattens' it into a vector of length 12 x 12 x 64 = 9216.\n",
        "\n",
        "- The `Dense` layer is a plain fully connected layer with 128 neurons. It takes the 9216-dimensional output vector from the previous layer (layer l-1) as the input and has 128 x 9216 (weights) + 128 (biases) =  1179776 trainable parameters. The output of this layer is a 128-dimensional vector.\n",
        "\n",
        "- The `Dropout` layer simply drops a few neurons.\n",
        "\n",
        "- Finally, we have a `Dense` softmax layer with 10 neurons which takes the 128-dimensional vector from the previous layer as input. It has 128 x 10 (weights) + 10 (biases) = 1290 trainable parameters.\n",
        "\n",
        "Thus, the total number of parameters are 1,199,882 all of which are trainable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgpuqt5Kycys"
      },
      "source": [
        "## 4. Fitting and Evaluating the Model\n",
        "\n",
        "Let's now compile and train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfO51K9YjlAf",
        "outputId": "7cfd410a-21b2-4117-e0d2-09e87a0ae8f6"
      },
      "source": [
        "# Defining the model parameters\n",
        "# optimizer: adam (SGD)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model\n",
        "history = model.fit(train_images, train_labels, epochs=10, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4930: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 9s 3ms/step - loss: 0.1324 - accuracy: 0.9601 - val_loss: 0.0428 - val_accuracy: 0.9858\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0484 - accuracy: 0.9851 - val_loss: 0.0311 - val_accuracy: 0.9904\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0364 - accuracy: 0.9882 - val_loss: 0.0305 - val_accuracy: 0.9903\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0280 - accuracy: 0.9908 - val_loss: 0.0350 - val_accuracy: 0.9900\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0224 - accuracy: 0.9927 - val_loss: 0.0318 - val_accuracy: 0.9907\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0190 - accuracy: 0.9941 - val_loss: 0.0304 - val_accuracy: 0.9919\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.0279 - val_accuracy: 0.9921\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0138 - accuracy: 0.9956 - val_loss: 0.0360 - val_accuracy: 0.9901\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.0408 - val_accuracy: 0.9895\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0109 - accuracy: 0.9963 - val_loss: 0.0371 - val_accuracy: 0.9910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R5JqZfLfGQL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "63a2c324-8ef9-4a3f-9b76-1368fed41ffb"
      },
      "source": [
        "# Plotting accuracy over train and test data against the Epoch value\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.7, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2ef67ead90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wV9X3/8dfnXPYKIsqKyqqgQUFERLaa6KNKJBhMjST6Q7DGGmqkuWi9pDVq0kiNTdNoazQ1RmjR2GhogjU1PlKtCAYfVRMWNV5QFBFlUXHlstx22XP5/P6Y2d3DMrscYIez7r6fj0zm+/3OfOd8zlmcz5mZM/M1d0dERKSzRKkDEBGR3kkJQkREIilBiIhIJCUIERGJpAQhIiKRlCBERCRSbAnCzOaZ2Ydm9koXy83M7jSzlWb2kpmdXLDsUjN7M5wujStGERHpWpxHEPcBU7pZfg4wMpxmAXcDmNlBwE3AqcApwE1mNjjGOEVEJEJsCcLdlwAbulllKnC/B54DDjSzw4DPAk+4+wZ33wg8QfeJRkREYpAq4WsPA9YU1BvCtq7ad2FmswiOPqiurp4watSoeCIVEemjli1b9pG710QtK2WC2GfuPgeYA1BXV+f19fUljkhE5OPFzN7palkpE8Ra4IiCem3YthaY2Kn9qf0WlYiUXC7vZHL5cArKrdmgns07rdk8ubyTcyef96Ac1nMF9bw72cJyLpjn8pDLt20D8vlgvXzYP5sPt9tpe4Wv2bY+wf8AcPeCctDe9rw7b/8/cDxY3rle0LetFrWdjn5B+zE1A5h93pge/zuUMkE8AlxhZvMJLkg3ufv7ZvY48P2CC9NnAzeUKkiRUnB3MjmnNZdnRyZHay7YmeXzkHcPp4JyVHu+i/IufTr1zXeznTCuTC5PJtxhZ/LeXm7NFe7YO3buwfrB+2mrZ3OF9WAbbfV8L3iGaMIglUiQSEDSjGSiY0qYkUoYZoYZwYQBbWXCctgaNljY1lHeuR8RyylYzwq2Q8F6za25WD6D2BKEmf2C4EhgiJk1EPwyKQ3g7j8Ffgt8DlgJbAdmhss2mNn3gKXhpm529+4udov0KHdnRzZPc2uO7ZkcLZkcOzLBzqs1m2dHNhfOg3pb246CtvZluY6+7fOwf2un9YNysJ3WXL79W2JvV5ZMkE4a6VSCdDJBWTJBKmmkk231jvIBZWnSibCeCvqVhcvSbdtpK6d2XpYqWDeVDHbQiYSRtIJy29TFDj1Z0GendZPBPJEIk4J17Kj7s9gShLtftJvlDnyji2XzgHlxxCV9Qz7vNGdybG/NhTvybEe5Ncf21mx7OViv8/Iczbv0ydHcmqU5k9vnb7BlSWNAKk91Mk91KseAZI6qVI7qZJ4DE3kqE1mqkjkq0jmqEjnKE1kqLJjKEzkqyFJm4USGNFmS5DGDBB58uwTMvKCeJwFYeGrC8GBHh4cTBfN8Qb1jOYXrmmPetm7bNsHKKkmUVWPlVcG8rBrSVbDLvArS1eE8bE+WdXwtll7vY32R+mNvx1bYug62vA9bPoDt68Hz4cKCY8udygUil0XU93hZ2zFzoqCe6Ki3lXdqJ6Kto5x3aM46zdk821vzbM842zNBeVsmz7ZWZ3trjm2ZPFt3ONsywY67tXUHmUyGTCZDayZDLpshk2klnwt2mClyJMJ5snBuwTyYcpRZnqqkc3DSqUg65UmoSOYpTzjliTzl5U5ZpVOWyJNOOGWWJ2050uakyJDyLCnPkPQMKc+QyLeSzGewfCuJfAbLtQZTPgPZHcEcIA+07s0/jk4sAZbc+e+z07zwb0UX60St23le0Lf971m4DpBpgcw2aN0O2eY9fB/JguSxm2TSXXu6ChLJYLIkJFJhORG2p8L2tnIiet04k1UuA9kWyO7omGeaC+qFU1fr7Ag+453qLcHfoLB+yGiY8UCPvwUliJ7mDjs2Bzv8LR+ECaCt/AFsCRPC1nXQurXU0e43CaA6nPZZMpz2Rj6cwv03iXS4w0hBIlFQTnXsUJJlwZQqg2Q5JAdAqrygvRyS6XBZ4XrpiPXKOm0vXDdV1mlZp36JvX3DMcvnIbM9mFq3hfPtHQlkp/Zulrdug62Nu7YT83k2K0wyncttSSbRKbEUrAtd77R9H68LJMshVRH8/dvm6YqwXAFVB3WUh4zc988ighJEsdyheWPBjr6bBBD1rSpdBQMPhQGHwmEnBvOB4TRgaDCvrgn+0RWefG4v+6717paFdfc8zZk8W5pb2dySZUtLhs3NGba2BNPmlgxbWlrZ0pJja0uGLS0Ztu3IsH1HlubWDK3ZXHgKIziVkWg/NZHvaDenOp2guixBVTpBVTivLjOq0kmq0gkq09Y+r0wZlWUJqlJGRdqoSCWoTCWoSEIyEb6PtiOpwp31TvPUrt8Su1032alPmBBk3yQSUD4gmHqae7CjbUsYhUkm0wz5LORzwY44n+tUznaUPV/kuvlwvrt18x1t7gU77XJIVRbs0Nt26pU77+RTFTsvT1V02kZFkBx6wb9PJYh8Pji10/atPjIBrAvachHnCsoP6NjBD6sr2Ol3SgDlA/fpcLYlk2Nzc4ZNzRmamjM0bS8oNwc7/U3bW9vrm8K2puYMmVzX38KSCWNQZZpBlRUcUJlm0MA0gw9Jc2RFioHlKQaUpxhQEcwHVqQYUJ5ur7ctq0onSSR0Xll6mFmwc01XAgeXOpp+SQli6wfwL6N3ba84sGMHf9RpMHBo9Lf+sh45acKqxq38Z/0aPtrSSlNzx46+qTnDpu0ZdmTzXfY1g4HlKQZVpTmwsoxBlWkOG1TJoKp0uPNPc2BlR/mAyjQHhssGlKf0aw0RiaQEUX0InHPrzglgwNDgkG8/WLe5hR8tfJNf1q8haUbNwPJgB16Z5ughA4KdesGOflDBzj3Y8ZcxoCJFUt/gRaSHKUEkU3DqrP3+sk3NGX76u7e49//eJpd3vnTqkVxx1khqBpbv91hERKIoQexnLZkcP3tmNT956i02t2SYOu5wrp18HEceXFXq0EREdqIEsZ9kc3kWLGvgRwvf5IPNLUw8robrPjuK4w8/oNShiYhEUoKImbvz+KsfcOvjK3ircRvjjzyQH804iU8erV9liEjvpgQRo2fe+oh/emwFf1yziU8cMoB7LpnA2ccP1a+GRORjQQkiBq+sbeKHj69gyRuNHDaogh9ecCLnnzyMVLL0N76IiBRLCaIHvbN+G7f97xv85o/vcWBVmm9/bjSXfOooKtK99DEJIiLdUILoAR9uaeHHT67kF394l1TS+Manj2HWGccwqDJd6tBERPaaEsQ+2NySYe6SVfzb02+TyeWZccoR/PVZIznkgP1zk52ISJyUIPZCSybHz597h7sWr2Tj9gznnngYf3P2cQwf0jOP3RAR6Q2UIPZALu/81/PBvQxrNzXzpyOHcN1nRzG2dlCpQxMR6XFKEEVwdxa+9iG3Pv46b6zbyrjaQdz6/07ktE8MKXVoIiKxUYLYjT+8vYF/eux1lr2zkaOHVPOTi0/mnBMO1b0MItLnKUF04fUPNvPDx1aw6PUPGXpAOf94/limTajVvQwi0m8oQXSyZsN2bn/iDR5+cS0Dy1N8a8oovnzacCrLdC+DiPQvShCh9Vt38ONFK3ng9++QMGPWGUfz9TM/waAq3csgIv1Tv08Q23Zkmfv0KuYuWUVLNs+FdbVcNelYDh2kexlEpH/r9wli644s9/xuFZ8eVcM3zz6OY2piGHxdRORjqN8niKEHVPC7v52ou59FRDrRT3JAyUFEJIIShIiIRIo1QZjZFDNbYWYrzez6iOVHmdmTZvaSmT1lZrUFy3Jm9mI4PRJnnCIisqvYrkGYWRK4C5gMNABLzewRd19esNptwP3u/jMzOwv4R+CScFmzu58UV3wiItK9OI8gTgFWuvsqd28F5gNTO61zPLAoLC+OWC4iIiUSZ4IYBqwpqDeEbYX+CJwflr8IDDSzg8N6hZnVm9lzZvaFGOMUEZEIpb5I/TfAmWb2AnAmsBbIhcuOcvc64M+BH5nZMZ07m9msMInUNzY27regRUT6gzgTxFrgiIJ6bdjWzt3fc/fz3X088O2wbVM4XxvOVwFPAeM7v4C7z3H3Onevq6mpieVNiIj0V3EmiKXASDMbYWZlwAxgp18jmdkQM2uL4QZgXtg+2MzK29YBTgcKL26LiEjMYksQ7p4FrgAeB14Dfunur5rZzWZ2XrjaRGCFmb0BDAX+IWwfDdSb2R8JLl7/oNOvn0REJGbm7qWOoUfU1dV5fX19qcMQEflYMbNl4fXeXZT6IrWIiPRSShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISKRYE4SZTTGzFWa20syuj1h+lJk9aWYvmdlTZlZbsOxSM3sznC6NM04REdlVbAnCzJLAXcA5wPHARWZ2fKfVbgPud/cTgZuBfwz7HgTcBJwKnALcZGaD44pVRER2FecRxCnASndf5e6twHxgaqd1jgcWheXFBcs/Czzh7hvcfSPwBDAlxlhFRKSTOBPEMGBNQb0hbCv0R+D8sPxFYKCZHVxkX8xslpnVm1l9Y2NjjwUuIiKlv0j9N8CZZvYCcCawFsgV29nd57h7nbvX1dTUxBWjiEi/lIpx22uBIwrqtWFbO3d/j/AIwswGABe4+yYzWwtM7NT3qRhjFRGRTuI8glgKjDSzEWZWBswAHilcwcyGmFlbDDcA88Ly48DZZjY4vDh9dtgmIiL7SWwJwt2zwBUEO/bXgF+6+6tmdrOZnReuNhFYYWZvAEOBfwj7bgC+R5BklgI3h20iIrKfmLuXOoYeUVdX5/X19aUOQ0TkY8XMlrl7XdSyUl+kFhGRXkoJQkREIilBiIhIJCUIERGJpAQhIiKRlCBERCSSEoSIiERSghARkUhKECIiEkkJQkREIu02QZjZ5wseqCciIv1EMTv+6cCbZvZDMxsVd0AiItI77DZBuPuXgPHAW8B9ZvZsOJLbwNijExGRkinq1JG7bwYWEIwrfRjB8KDPm9mVMcYmIiIlVMw1iPPM7GGCEd3SwCnufg4wDvhmvOGJiEipFDPk6AXA7e6+pLDR3beb2WXxhCUiIqVWTIKYDbzfVjGzSmCou6929yfjCkxEREqrmGsQvwLyBfVc2CYiIn1YMQki5e6tbZWwXBZfSCIi0hsUkyAazey8toqZTQU+ii8kERHpDYq5BvFV4AEz+1fAgDXAX8QalYiIlNxuE4S7vwV80swGhPWtsUclIiIlV8wRBGb2Z8AYoMLMAHD3m2OMS0RESqyYG+V+SvA8pisJTjFNA46KOS4RESmxYi5Sn+bufwFsdPe/Bz4FHBtvWCIiUmrFJIiWcL7dzA4HMgTPYxIRkT6smGsQvzGzA4FbgecBB+bGGpWIiJRct0cQ4UBBT7r7Jnd/iODawyh3/24xGzezKWa2wsxWmtn1EcuPNLPFZvaCmb1kZp8L24ebWbOZvRhOP92L9yYiIvug2yMId8+b2V0E40Hg7juAHcVs2MySwF3AZKABWGpmj7j78oLVvgP80t3vNrPjgd8Cw8Nlb7n7SXvyZkREpOcUcw3iSTO7wNp+31q8U4CV7r4qfDzHfGBqp3UcOCAsDwLe28PXEBGRmBSTIP6K4OF8O8xss5ltMbPNRfQbRnDXdZuGsK3QbOBLZtZAcPRQOADRiPDU0+/M7E+jXiAc2a7ezOobGxuLCElERIpVzJCjA9094e5l7n5AWD9gd/2KdBFwn7vXAp8D/iO87vE+cKS7jweuBR40s11e093nuHudu9fV1NT0UEgiIgJF/IrJzM6Iau88gFCEtcARBfXasK3QZcCUcHvPmlkFMMTdPyS81uHuy8zsLYJ7L+p3F6+IiPSMYn7m+rcF5QqCawvLgLN2028pMNLMRhAkhhnAn3da511gEnCfmY0Ot99oZjXABnfPmdnRwEhgVRGxiohIDynmYX2fL6yb2RHAj4rolzWzK4DHgSQwz91fNbObgXp3f4RgTOu5ZnYNwQXrL7u7h0ctN5tZhmCwoq+6+4Y9fXMiIrL3zN33rEPwa6ZX3f34eELaO3V1dV5frzNQIiJ7wsyWuXtd1LJirkH8mODbPQQXtU8iuKNaRET6sGKuQRR+Lc8Cv3D3/4spHhER6SWKSRALgBZ3z0Fwh7SZVbn79nhDExGRUirqTmqgsqBeCSyMJxwREektikkQFYXDjIblqvhCEhGR3qCYBLHNzE5uq5jZBKA5vpBERKQ3KOYaxNXAr8zsPYIhRw8lGIJURET6sGJulFtqZqOA48KmFe6eiTcsEREptd2eYjKzbwDV7v6Ku78CDDCzr8cfmoiIlFIx1yAud/dNbRV33whcHl9IIiLSGxSTIJKFgwWFI8WVxReSiIj0BsVcpH4M+E8zuyes/xXwP/GFJCIivUExCeJbwCzgq2H9JYJfMomISB9WzIhyeeD3wGqCsSDOAl6LNywRESm1Lo8gzOxYgiFBLwI+Av4TwN0/vX9CExGRUuruFNPrwNPAue6+EiAc2EdERPqB7k4xnQ+8Dyw2s7lmNongTmoREekHukwQ7v5rd58BjAIWEzxy4xAzu9vMzt5fAYqISGkUc5F6m7s/GI5NXQu8QPDLJhER6cOKuVGunbtvdPc57j4proBERKR32KMEISIi/YcShIiIRFKCEBGRSEoQIiISSQlCREQiKUGIiEgkJQgREYkUa4IwsylmtsLMVprZ9RHLjzSzxWb2gpm9ZGafK1h2Q9hvhZl9Ns44RURkV8WMB7FXwpHn7gImAw3AUjN7xN2XF6z2HeCX7n63mR0P/BYYHpZnAGOAw4GFZnasu+fiildERHYW5xHEKcBKd1/l7q3AfGBqp3UcOCAsDwLeC8tTgfnuvsPd3wZWhtsTEZH9JM4EMQxYU1BvCNsKzQa+ZGYNBEcPV+5BX8xslpnVm1l9Y2NjT8UtIiKU/iL1RcB97l4LfA74DzMrOqbwuVB17l5XU1MTW5AiIv1RbNcggLXAEQX12rCt0GXAFAB3f9bMKoAhRfYVEZEYxXkEsRQYaWYjzKyM4KLzI53WeReYBGBmo4EKoDFcb4aZlZvZCGAk8IcYYxURkU5iO4Jw96yZXQE8DiSBee7+qpndDNS7+yPAN4G54VCmDnzZ3R141cx+CSwHssA39AsmEZH9y4L98cdfXV2d19fXlzoMEZGPFTNb5u51UctKfZFaRER6KSUIERGJpAQhIiKRlCBERCSSEoSIiERSghARkUhKECIiEkkJQkREIilBiIhIJCUIERGJpAQhIiKRlCBERCSSEoSIiERSghARkUhKECIiEkkJQkREIilBiIhIJCUIERGJpAQhIiKRlCBERCSSEoSIiERSghARkUhKECIiEkkJQkREIilBiIhIJCUIERGJpAQhIiKRYk0QZjbFzFaY2Uozuz5i+e1m9mI4vWFmmwqW5QqWPRJnnCIisqtUXBs2syRwFzAZaACWmtkj7r68bR13v6Zg/SuB8QWbaHb3k+KKT0TilclkaGhooKWlpdShCFBRUUFtbS3pdLroPrElCOAUYKW7rwIws/nAVGB5F+tfBNwUYzwish81NDQwcOBAhg8fjpmVOpx+zd1Zv349DQ0NjBgxouh+cZ5iGgasKag3hG27MLOjgBHAooLmCjOrN7PnzOwL8YUpInFoaWnh4IMPVnLoBcyMgw8+eI+P5uI8gtgTM4AF7p4raDvK3dea2dHAIjN72d3fKuxkZrOAWQBHHnnk/otWRIqi5NB77M3fIs4jiLXAEQX12rAtygzgF4UN7r42nK8CnmLn6xNt68xx9zp3r6upqemJmEVEJBRnglgKjDSzEWZWRpAEdvk1kpmNAgYDzxa0DTaz8rA8BDidrq9diIhIDGI7xeTuWTO7AngcSALz3P1VM7sZqHf3tmQxA5jv7l7QfTRwj5nlCZLYDwp//SQi0ptks1lSqd5yxr7nxPqO3P23wG87tX23U312RL9ngLFxxiYi+8/f/+ZVlr+3uUe3efzhB3DT58fsdr0vfOELrFmzhpaWFq666ipmzZrFY489xo033kgul2PIkCE8+eSTbN26lSuvvJL6+nrMjJtuuokLLriAAQMGsHXrVgAWLFjAo48+yn333ceXv/xlKioqeOGFFzj99NOZMWMGV111FS0tLVRWVnLvvfdy3HHHkcvl+Na3vsVjjz1GIpHg8ssvZ8yYMdx55538+te/BuCJJ57gJz/5CQ8//HCPfkb7qu+lPBGRAvPmzeOggw6iubmZP/mTP2Hq1KlcfvnlLFmyhBEjRrBhwwYAvve97zFo0CBefvllADZu3LjbbTc0NPDMM8+QTCbZvHkzTz/9NKlUioULF3LjjTfy0EMPMWfOHFavXs2LL75IKpViw4YNDB48mK9//es0NjZSU1PDvffey1/+5V/G+jnsDSUIEYldMd/043LnnXe2fzNfs2YNc+bM4Ywzzmi/H+Cggw4CYOHChcyfP7+93+DBg3e77WnTppFMJgFoamri0ksv5c0338TMyGQy7dv96le/2n4Kqu31LrnkEn7+858zc+ZMnn32We6///4eesc9RwlCRPqsp556ioULF/Lss89SVVXFxIkTOemkk3j99deL3kbhz0M730dQXV3dXv67v/s7Pv3pT/Pwww+zevVqJk6c2O12Z86cyec//3kqKiqYNm1ar7yGoYf1iUif1dTUxODBg6mqquL111/nueeeo6WlhSVLlvD2228DtJ9imjx5MnfddVd737ZTTEOHDuW1114jn893e42gqamJYcOCe4Hvu+++9vbJkydzzz33kM1md3q9ww8/nMMPP5xbbrmFmTNn9tyb7kFKECLSZ02ZMoVsNsvo0aO5/vrr+eQnP0lNTQ1z5szh/PPPZ9y4cUyfPh2A73znO2zcuJETTjiBcePGsXjxYgB+8IMfcO6553Laaadx2GGHdfla1113HTfccAPjx49vTwYAX/nKVzjyyCM58cQTGTduHA8++GD7sosvvpgjjjiC0aNHx/QJ7Bvb+delH191dXVeX19f6jBEJPTaa6/12h1fb3HFFVcwfvx4Lrvssv3yelF/EzNb5u51Uev3vpNeIiL9wIQJE6iuruaf//mfSx1Kl5QgRERKYNmyZaUOYbd0DUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiEhowIABpQ6hV9HPXEUkfv9zPXzwcs9u89CxcM4PenabvURvGV9CRxAi0mddf/31Oz1fafbs2dxyyy1MmjSJk08+mbFjx/Lf//3fRW1r69atXfa7//772x+lcckllwCwbt06vvjFLzJu3DjGjRvHM888w+rVqznhhBPa+912223Mnj0bgIkTJ3L11VdTV1fHHXfcwW9+8xtOPfVUxo8fz2c+8xnWrVvXHsfMmTMZO3YsJ554Ig899BDz5s3j6quvbt/u3Llzueaaa/b6c2vn7n1imjBhgotI77F8+fJSh+DPP/+8n3HGGe310aNH+7vvvutNTU3u7t7Y2OjHHHOM5/N5d3evrq7ucluZTCay3yuvvOIjR470xsZGd3dfv369u7tfeOGFfvvtt7u7ezab9U2bNvnbb7/tY8aMad/mrbfe6jfddJO7u5955pn+ta99rX3Zhg0b2uOaO3euX3vtte7uft111/lVV12103pbtmzxo48+2ltbW93d/VOf+pS/9NJLu7yHqL8JwQifkfvV0h/DiIjEZPz48Xz44Ye89957NDY2MnjwYA499FCuueYalixZQiKRYO3ataxbt45DDz202225OzfeeOMu/RYtWsS0adMYMmQI0DHew6JFi9rHeEgmkwwaNGi3gxC1PTgQgsGIpk+fzvvvv09ra2v7+BVdjVtx1lln8eijjzJ69GgymQxjx+77oJxKECLSp02bNo0FCxbwwQcfMH36dB544AEaGxtZtmwZ6XSa4cOH7zLOQ5S97VcolUqRz+fb692NL3HllVdy7bXXct555/HUU0+1n4rqyle+8hW+//3vM2rUqB57fLiuQYhInzZ9+nTmz5/PggULmDZtGk1NTRxyyCGk02kWL17MO++8U9R2uup31lln8atf/Yr169cDHeM9TJo0ibvvvhuAXC5HU1MTQ4cO5cMPP2T9+vXs2LGDRx99tNvXaxtf4mc/+1l7e1fjVpx66qmsWbOGBx98kIsuuqjYj6dbShAi0qeNGTOGLVu2MGzYMA477DAuvvhi6uvrGTt2LPfffz+jRo0qajtd9RszZgzf/va3OfPMMxk3bhzXXnstAHfccQeLFy9m7NixTJgwgeXLl5NOp/nud7/LKaecwuTJk7t97dmzZzNt2jQmTJjQfvoKuh63AuDCCy/k9NNPL2q41GJoPAgRiYXGg9j/zj33XK655homTZoUuXxPx4PQEYSIyMfcpk2bOPbYY6msrOwyOewNXaQWESnw8ssvt9/L0Ka8vJzf//73JYpo9w488EDeeOONHt+uEoSIxMbdMbNSh7FHxo4dy4svvljqMHrc3lxO0CkmEYlFRUUF69ev36sdk/Qsd2f9+vVUVFTsUT8dQYhILGpra2loaKCxsbHUoQhBwq6trd2jPkoQIhKLdDrdfvevfDzFeorJzKaY2QozW2lm10csv93MXgynN8xsU8GyS83szXC6NM44RURkV7EdQZhZErgLmAw0AEvN7BF3X962jrtfU7D+lcD4sHwQcBNQBziwLOzb/YNMRESkx8R5BHEKsNLdV7l7KzAfmNrN+hcBvwjLnwWecPcNYVJ4ApgSY6wiItJJnNcghgFrCuoNwKlRK5rZUcAIYFE3fYdF9JsFzAqrW81sxT7EOwT4aB/69yX6LHamz2Nn+jw69IXP4qiuFvSWi9QzgAXuntuTTu4+B5jTEwGYWX1Xt5v3N/osdqbPY2f6PDr09c8izlNMa4EjCuq1YVuUGXScXtrTvir6+9sAAARTSURBVCIiEoM4E8RSYKSZjTCzMoIk8EjnlcxsFDAYeLag+XHgbDMbbGaDgbPDNhER2U9iO8Xk7lkzu4Jgx54E5rn7q2Z2M8EQd23JYgYw3wtut3T3DWb2PYIkA3Czu2+IK9ZQj5yq6iP0WexMn8fO9Hl06NOfRZ953LeIiPQsPYtJREQiKUGIiEikfp8gdvc4kP7EzI4ws8VmttzMXjWzq0odU6mZWdLMXjCzrgcP7ifM7EAzW2Bmr5vZa2b2qVLHVEpmdk3438krZvYLM9uzR6V+DPTrBFHwOJBzgOOBi8zs+NJGVVJZ4JvufjzwSeAb/fzzALgKeK3UQfQSdwCPufsoYBz9+HMxs2HAXwN17n4CwQ9xZpQ2qp7XrxMEe/44kD7N3d939+fD8haCHcAud7D3F2ZWC/wZ8G+ljqXUzGwQcAbw7wDu3urum7rv1eelgEozSwFVwHsljqfH9fcEUdQjPfojMxtO8PDE3jvOYvx+BFwH5EsdSC8wAmgE7g1Puf2bmVWXOqhScfe1wG3Au8D7QJO7/29po+p5/T1BSAQzGwA8BFzt7ptLHU8pmNm5wIfuvqzUsfQSKeBk4G53Hw9sA/rtNbvwBt6pBInzcKDazL5U2qh6Xn9PEHqkRydmliZIDg+4+3+VOp4SOh04z8xWE5x6PMvMfl7akEqqAWhw97YjygUECaO/+gzwtrs3unsG+C/gtBLH1OP6e4Io6nEg/YUFo8v/O/Cau/9LqeMpJXe/wd1r3X04wb+LRe7e574hFsvdPwDWmNlxYdMkYHk3Xfq6d4FPmllV+N/NJPrgRfve8jTXkujqcSAlDquUTgcuAV42sxfDthvd/bcljEl6jyuBB8IvU6uAmSWOp2Tc/fdmtgB4nuDXfy/QBx+7oUdtiIhIpP5+iklERLqgBCEiIpGUIEREJJIShIiIRFKCEBGRSEoQInvAzHJm9mLB1GN3E5vZcDN7pae2J7Kv+vV9ECJ7odndTyp1ECL7g44gRHqAma02sx+a2ctm9gcz+0TYPtzMFpnZS2b2pJkdGbYPNbOHzeyP4dT2mIakmc0Nxxn4XzOrLNmbkn5PCUJkz1R2OsU0vWBZk7uPBf6V4EmwAD8GfubuJwIPAHeG7XcCv3P3cQTPNGq7g38kcJe7jwE2ARfE/H5EuqQ7qUX2gJltdfcBEe2rgbPcfVX4wMMP3P1gM/sIOMzdM2H7++4+xMwagVp331GwjeHAE+4+Mqx/C0i7+y3xvzORXekIQqTneBflPbGjoJxD1wmlhJQgRHrO9IL5s2H5GTqGorwYeDosPwl8DdrHvR60v4IUKZa+nYjsmcqCJ91CMEZz209dB5vZSwRHAReFbVcSjML2twQjsrU9AfUqYI6ZXUZwpPA1gpHJRHoNXYMQ6QHhNYg6d/+o1LGI9BSdYhIRkUg6ghARkUg6ghARkUhKECIiEkkJQkREIilBiIhIJCUIERGJ9P8Bt58smvAY2nkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SVlSp_Jsycyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee4c056-5aa6-4787-a1c4-d63b534d3907"
      },
      "source": [
        "# Evaluating the test accuracy\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(test_acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 1s - loss: 0.0371 - accuracy: 0.9910\n",
            "0.9909999966621399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHgW5yxoycyv"
      },
      "source": [
        "The final loss (on test data) is about 0.04 and the accuracy is 99.1%."
      ]
    }
  ]
}