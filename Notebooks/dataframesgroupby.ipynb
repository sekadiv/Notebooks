{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions\n",
    "\n",
    "GroupBy allows you to group rows together based off some column value. \n",
    "\n",
    "Once you've performed the GroupBy operation you can use an aggregate function off that data. \n",
    "\n",
    "An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May take a little while on a local computer\n",
    "spark = SparkSession.builder.appName(\"GroupBy\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the customer sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('sales_info.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|  Company|Quarter|Sales|\n",
      "+---------+-------+-----+\n",
      "|   Google|     Q1|  200|\n",
      "|   Google|     Q2|  120|\n",
      "|   Google|     Q3|  340|\n",
      "|   Google|     Q4|  300|\n",
      "|Microsoft|     Q2|  300|\n",
      "|Microsoft|     Q3|  124|\n",
      "|Microsoft|     Q4|  243|\n",
      "|Microsoft|     Q1|  330|\n",
      "| Facebook|     Q3|  870|\n",
      "| Facebook|     Q2|  750|\n",
      "|    Apple|     Q1|  650|\n",
      "|    Apple|     Q2|  550|\n",
      "|    Apple|     Q3|  750|\n",
      "|    Apple|     Q4|  850|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's group together by company!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fa057cc6310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Company')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a GroupedData object, off of which you can all various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|  Company|avg(Sales)|\n",
      "+---------+----------+\n",
      "|   Google|     240.0|\n",
      "|Microsoft|    249.25|\n",
      "|    Apple|     700.0|\n",
      "| Facebook|     810.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "df.groupby('Company').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  Company|count|\n",
      "+---------+-----+\n",
      "|   Google|    4|\n",
      "|Microsoft|    4|\n",
      "|    Apple|    4|\n",
      "| Facebook|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count\n",
    "df.groupby('Company').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|  Company|max(Sales)|\n",
      "+---------+----------+\n",
      "|   Google|       340|\n",
      "|Microsoft|       330|\n",
      "|    Apple|       850|\n",
      "| Facebook|       870|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max\n",
    "df.groupby('Company').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|  Company|min(Sales)|\n",
      "+---------+----------+\n",
      "|   Google|       120|\n",
      "|Microsoft|       124|\n",
      "|    Apple|       550|\n",
      "| Facebook|       750|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min\n",
    "df.groupby('Company').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|  Company|sum(Sales)|\n",
      "+---------+----------+\n",
      "|   Google|       960|\n",
      "|Microsoft|       997|\n",
      "|    Apple|      2800|\n",
      "| Facebook|      1620|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sum\n",
    "df.groupby('Company').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "There are a variety of functions you can import from pyspark.sql.functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   12|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Sales|\n",
      "+-----+\n",
      "|  200|\n",
      "|  120|\n",
      "|  340|\n",
      "|  300|\n",
      "|  300|\n",
      "|  124|\n",
      "|  243|\n",
      "|  330|\n",
      "|  870|\n",
      "|  750|\n",
      "|  650|\n",
      "|  550|\n",
      "|  750|\n",
      "|  850|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(Sales)|\n",
      "+----------+\n",
      "|     455.5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|stddev_samp(Sales)|\n",
      "+------------------+\n",
      "| 271.4634838738409|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stddev('Sales')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporaryly chinging column name\n",
    "\n",
    "using the methods on the data gives wired column names, use the .alias() method to change the column name temporaryly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        std_sales|\n",
      "+-----------------+\n",
      "|271.4634838738409|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stddev('Sales').alias('std_sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select(stddev('Sales').alias('std_sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        std_sales|\n",
      "+-----------------+\n",
      "|271.4634838738409|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rounding off digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|std_sales|\n",
      "+---------+\n",
      "|   271.46|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(format_number('std_sales',2).alias('std_sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order By\n",
    "\n",
    "As the name suggests this method is used to sort the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|  Company|Quarter|Sales|\n",
      "+---------+-------+-----+\n",
      "|   Google|     Q2|  120|\n",
      "|Microsoft|     Q3|  124|\n",
      "|   Google|     Q1|  200|\n",
      "|Microsoft|     Q4|  243|\n",
      "|   Google|     Q4|  300|\n",
      "|Microsoft|     Q2|  300|\n",
      "|Microsoft|     Q1|  330|\n",
      "|   Google|     Q3|  340|\n",
      "|    Apple|     Q2|  550|\n",
      "|    Apple|     Q1|  650|\n",
      "|    Apple|     Q3|  750|\n",
      "| Facebook|     Q2|  750|\n",
      "|    Apple|     Q4|  850|\n",
      "| Facebook|     Q3|  870|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the orderby sorts data in accending order, for desending we need to use syntax desc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "| Company|Quarter|Sales|\n",
      "+--------+-------+-----+\n",
      "|Facebook|     Q3|  870|\n",
      "|   Apple|     Q4|  850|\n",
      "|Facebook|     Q2|  750|\n",
      "+--------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['Sales'].desc()).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
