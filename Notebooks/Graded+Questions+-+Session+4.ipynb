{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "## Housing Case Study\n",
    "\n",
    "#### Problem Statement:\n",
    "\n",
    "Consider a real estate company that has a dataset containing the prices of properties in the Delhi region. It wishes to use the data to optimise the sale prices of the properties based on important factors such as area, bedrooms, parking, etc.\n",
    "\n",
    "Essentially, the company wants —\n",
    "\n",
    "\n",
    "- To identify the variables affecting house prices, e.g. area, number of rooms, bathrooms, etc.\n",
    "\n",
    "- To create a linear model that quantitatively relates house prices with variables such as number of rooms, area, number of bathrooms, etc.\n",
    "\n",
    "- To know the accuracy of the model, i.e. how well these variables can predict house prices.\n",
    "\n",
    "**So interpretation is important!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Reading and Understanding the Data\n",
    "\n",
    "Let us first import NumPy and Pandas and read the housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"C:/Users/Phanendra.varma.UPGRAD/Downloads/Housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "\n",
    "Inspect the various aspects of the housing dataset and identify the mean of the area feature in the dataset\n",
    "\n",
    "- 5150\n",
    "- 545\n",
    "- 510\n",
    "- 296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualising the Data\n",
    "\n",
    "Let's now spend some time doing what is arguably the most important step - **understanding the data**.\n",
    "- If there is some obvious multicollinearity going on, this is the first place to catch it\n",
    "- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n",
    "\n",
    "We'll visualise our data using `matplotlib` and `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Numeric Variables\n",
    "\n",
    "Let's make a pairplot of all the numeric variables. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "\n",
    "Identify the plot that best represents the plot between area and price column\n",
    "\n",
    "1. ---Refer to the Image provided on Platform---\n",
    "2. ---Refer to the Image provided on Platform---\n",
    "3. ---Refer to the Image provided on Platform---\n",
    "4. ---Refer to the Image provided on Platform---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Categorical Variables\n",
    "\n",
    "As you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "\n",
    "How does a box plot between price and mainroad looks like\n",
    "\n",
    "1. ---Refer to the Image provided on Platform---\n",
    "2. ---Refer to the Image provided on Platform---\n",
    "3. ---Refer to the Image provided on Platform---\n",
    "4. ---Refer to the Image provided on Platform---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise some of these categorical features parallely by using the `hue` argument. Below is the plot for `furnishingstatus` with `airconditioning` as the hue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "sns.boxplot(x = 'furnishingstatus', y = 'price', hue = 'airconditioning', data = housing)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see that your dataset has many columns with values as 'Yes' or 'No'.\n",
    "\n",
    "- But in order to fit a regression line, we would need numerical values and not string. Hence, we need to convert them to 1s and 0s, where 1 is a 'Yes' and 0 is a 'No'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4\n",
    "\n",
    "Identify the columns that can be part of my varlist\n",
    "\n",
    "- guestroom\n",
    "- basement\n",
    "- hotwaterheating\n",
    "- bathrooms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here ------ Define the List of variables to map\n",
    "\n",
    "varlist =  []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the map function\n",
    "def binary_map(x):\n",
    "    return x.map({'yes': 1, \"no\": 0})\n",
    "\n",
    "# Applying the function to the housing list\n",
    "housing[varlist] = housing[varlist].apply(binary_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the housing dataframe now\n",
    "\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `furnishingstatus` has three levels. We need to convert these levels into integer as well. \n",
    "\n",
    "For this, we will use something called `dummy variables`.\n",
    "\n",
    "You can read more about dummy variable here - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummy variables for the feature 'furnishingstatus' and store it in a new variable - 'status'\n",
    "status = pd.get_dummies(housing['furnishingstatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the dataset 'status' looks like\n",
    "status.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you don't need three columns. You can drop the `furnished` column, as the type of furnishing can be identified with just the last two columns where — \n",
    "- `00` will correspond to `furnished`\n",
    "- `01` will correspond to `unfurnished`\n",
    "- `10` will correspond to `semi-furnished`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the first column from status df using 'drop_first = True'\n",
    "status = pd.get_dummies(housing['furnishingstatus'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to the original housing dataframe\n",
    "housing = pd.concat([housing, status], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'furnishingstatus' as we have created the dummies for it\n",
    "housing.drop(['furnishingstatus'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5\n",
    "\n",
    "Let's say another categorical column has five different labels, now using dummy variables on that column would add a minimum of ___ columns to my dataset \n",
    "\n",
    "- 2\n",
    "- 6\n",
    "- 5\n",
    "- 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Splitting the Data into Training and Testing Sets\n",
    "\n",
    "As you know, the first basic step for regression is performing a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We specify this so that the train and test data set always have the same rows, respectively\n",
    "np.random.seed(0)\n",
    "df_train, df_test = train_test_split(housing, train_size = 0.7, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling the Features \n",
    "\n",
    "In Linear Regression, scaling doesn't impact your model. Here we can see that except for `area`, all the columns have small integer values. So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As you know, there are two common ways of rescaling:\n",
    "\n",
    "1. Min-Max scaling \n",
    "2. Standardisation (mean-0, sigma-1) \n",
    "\n",
    "This time, we will use MinMax scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\n",
    "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']\n",
    "\n",
    "df_train[num_vars] = scaler.fit_transform(df_train[num_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6\n",
    "\n",
    "Identify the correlation coefficient between area and price column\n",
    "\n",
    "- 0.53\n",
    "- 0.32\n",
    "- 0.52\n",
    "- 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here --- check the correlation coefficients to see which variables are highly correlated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, `area` seems to the correlated to `price` the most. Let's see a pairplot for `area` vs `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick `area` as the first variable and we'll try to fit a regression line to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing into X and Y sets for the model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.pop('price')\n",
    "X_train = df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Building a linear model\n",
    "\n",
    "Build a linear regression model on the training data using `statsmodels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7\n",
    "\n",
    "Build a linear regression model to predict price of a house using two input variables that have the highest correlation with price. Report r2 score of the model?\n",
    "\n",
    "- 0.681\n",
    "- 0.480\n",
    "- 0.759\n",
    "- 0.915\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding all the variables to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8\n",
    "\n",
    "Build a linear regression model (Use lr1) to predict price of a house by considering all features of the housing dataframe.By observing the summary statistics identify which of these variables are insignificant considering level of significance to be 0.05?\n",
    "\n",
    "- Parking\n",
    "- stories\n",
    "- main road\n",
    "- semifurnished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here\n",
    "# Build a linear model as lr_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(lr_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the p-values, it looks like some of the variables aren't really significant (in the presence of other variables). We could simply drop the variable with the highest, non-significant p value. A better way would be to supplement this with the VIF information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking VIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the variable and updating the model\n",
    "\n",
    "As you can see from the summary statistics and the VIF dataframe, some variables are insignificant. Creating a linear regression model 2 by dropping the variable that has the highest VIF and that is least significant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping highly correlated variables and insignificant variables\n",
    "X = X_train\n",
    "X = X.drop('semi-furnished', 1)\n",
    "X = X.drop('bedrooms', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a second fitted model as lr-2\n",
    "\n",
    "X_train_lm = sm.add_constant(X)\n",
    "\n",
    "lr_2 = sm.OLS(y_train, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "print(lr_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the VIFs again for the new model\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the variable and updating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, all the VIF values are now under 5. But from the summary statistics, we can still see some of them have a slightly higher p-value, We should drop those variable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the variable with the p value = 0.030\n",
    "\n",
    "X = X_train\n",
    "X = X.drop('basement', 1)\n",
    "X = X.drop('semi-furnished', 1)\n",
    "X = X.drop('bedrooms', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a third fitted model as lr_3\n",
    "\n",
    "X_train_lm = sm.add_constant(X)\n",
    "lr_3 = sm.OLS(y_train, X_train_lm).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary of the model\n",
    "print(lr_3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Residual Analysis of the train data\n",
    "\n",
    "So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using the final dataframe that contains the necessary columns (list of columns used for building model 3) you can make predictions on the training dataset\n",
    "y_train_price = lr_3.predict(X_train_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_price), bins = 20)\n",
    "fig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \n",
    "plt.xlabel('Errors', fontsize = 18)                         # X-label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
