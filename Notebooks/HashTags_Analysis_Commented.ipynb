{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trending Hashtag Analysis on Twitter live data using Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import sparkContext & StreamingContext from PySpark library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Here you need to have same Python version on your local machine adn on worker node i.e. EC2. here both should have python3.\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/bin/python3\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")\n",
    "os.environ[\"PYTHONIOENCODING\"] = \"utf8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a **sparkContext** with AppName \"TwitterStreaming\".<br>\n",
    "- Setting the LogLevel of SparkContext to ERROR. This will not print all the logs which are INFO or WARN level.<br>\n",
    "- Create Spark Streaming Context using SC (spark context). Parameter 10 is the batch interval. <br>\n",
    "Every 10 second the analysis will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\",\"TwitterStreaming\")\n",
    "sc.setLogLevel('ERROR')\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to socket broker using ssc (spark streaming context)<br>\n",
    "Host : \"127.0.0.1\" (localhost) & port : 7777 (It can be anything but it has to be same in both the notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_data = ssc.socketTextStream(\"127.0.0.1\", 7777)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint-dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window function parameter sets the Window length. All the analysis will be done on tweets stored for 20 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = stream_data.window(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Stream:\n",
    "1. Receives tweet message, stored in lines. **Input DStream**\n",
    "2. splits the messages into words. **Apply transformation on DStream : flatMap**\n",
    "3. filters all the words which start with a hashtag(#). **transformation : filter**\n",
    "4. converts the words to lowercase. **transformation : map**\n",
    "5. maps each tag to (word, 1). **transformation : map**\n",
    "6. then reduces and counts occurrences of each hash tag. (action : reduceByKey) hashtags = **output DStream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the hashtags based on the counts in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final analysis: Most popular hashtags on streaming twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Spark Streaming:\n",
    "Spark Streaming code we have written till now will not execute, untill we start the ssc.<br>\n",
    "ssc.start() will start the spark streaming context. This is the Action for the whole code. <br>\n",
    "Now it'll create the lineage & DAG & do the lazy evaluation & start running the whole sequesnce of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**awaitTermination()** is very important to stop the SSC.<br> \n",
    "When we kill this python process then this signal will be sent to awaitTermination() function.<br> \n",
    "It will finally stop the spark streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
