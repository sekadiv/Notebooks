{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv('s3a://linear-regression-mlc/train.csv', header=True, inferSchema=True)\n",
    "# Using a smaller dataset of 10M rows\n",
    "df = df.limit(1_000_000)\n",
    "df = df.dropna()\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [\n",
    "    'pickup_latitude',\n",
    "    'pickup_longitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count'\n",
    "]\n",
    "assembler = VectorAssembler(inputCols = inputCols , outputCol = 'features')\n",
    "dataset = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'fare_amount')\n",
    "model = lr.fit(dataset)\n",
    "summary = model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({'pickup_latitude':'max'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NYC lies between 73 and 75 degrees West, and 40 and 42 degrees north\n",
    "\n",
    "TOP, BOTTOM, LEFT, RIGHT = 42, 40 ,-75, -73\n",
    "df = df.filter(df['pickup_latitude'] >= BOTTOM)\n",
    "df = df.filter(df['pickup_latitude'] <= TOP)\n",
    "df = df.filter(df['pickup_longitude'] <= RIGHT)\n",
    "df = df.filter(df['pickup_longitude'] >= LEFT)\n",
    "\n",
    "df = df.filter(df['dropoff_latitude'] >= BOTTOM)\n",
    "df = df.filter(df['dropoff_latitude'] <= TOP)\n",
    "df = df.filter(df['dropoff_longitude'] <= RIGHT)\n",
    "df = df.filter(df['dropoff_longitude'] >= LEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({'passenger_count':'min'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.filter(df['passenger_count']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({'fare_amount':'min'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.filter(df['fare_amount']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [\n",
    "    'pickup_latitude',\n",
    "    'pickup_longitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count'\n",
    "]\n",
    "assembler = VectorAssembler(inputCols = inputCols, outputCol = 'featuresClean')\n",
    "dataset = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'featuresClean', labelCol = 'fare_amount')\n",
    "model = lr.fit(dataset)\n",
    "summary = model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(('pickup_datetime')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn('datetime',df['pickup_datetime'].substr(0,19))\n",
    "df.select('datetime').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('timestamp', to_timestamp(df['datetime']))\n",
    "df.select('timestamp').show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,month,dayofweek, hour\n",
    "\n",
    "df = df.withColumn('year', year(df['timestamp']))\n",
    "df = df.withColumn('month', month(df['timestamp']))\n",
    "df = df.withColumn('dayofweek', dayofweek(df['timestamp']))\n",
    "df = df.withColumn('hour', hour(df['timestamp']))\n",
    "df.select('year','month', 'dayofweek', 'hour').show(truncate =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [\n",
    "    'pickup_latitude',\n",
    "    'pickup_longitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count', 'year', 'month', 'dayofweek' , 'hour'\n",
    "]\n",
    "assembler = VectorAssembler(inputCols = inputCols, outputCol = 'featuresCleanWithDate')\n",
    "dataset = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'featuresCleanWithDate', labelCol = 'fare_amount')\n",
    "model = lr.fit(dataset)\n",
    "summary = model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp\n",
    "df = df.withColumn('NYTime', from_utc_timestamp(df['timestamp'], 'EST'))\n",
    "df.select('NYTime').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,month,dayofweek, hour\n",
    "\n",
    "df = df.withColumn('year', year(df['NYTime']))\n",
    "df = df.withColumn('month', month(df['NYTime']))\n",
    "df = df.withColumn('dayofweek', dayofweek(df['NYTime']))\n",
    "df = df.withColumn('hour', hour(df['NYTime']))\n",
    "df.select('year','month', 'dayofweek', 'hour').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [\n",
    "    'pickup_latitude',\n",
    "    'pickup_longitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count', 'year', 'month', 'dayofweek' , 'hour'\n",
    "]\n",
    "assembler = VectorAssembler(inputCols = inputCols, outputCol = 'featuresCleanWithDate')\n",
    "dataset = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'featuresCleanWithDate', labelCol = 'fare_amount')\n",
    "model = lr.fit(dataset)\n",
    "summary = model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df['pickup_longitude']\n",
    "y1 = df['pickup_latitude']\n",
    "x2 = df['dropoff_longitude']\n",
    "y2 = df['dropoff_latitude']\n",
    "\n",
    "from pyspark.sql.functions import abs as psabs\n",
    "df = df.withColumn('l1', psabs(x1 - x2) + psabs(y1 - y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [\n",
    "    'pickup_latitude',\n",
    "    'pickup_longitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count', 'year', 'month', 'dayofweek' , 'hour', 'l1'\n",
    "]\n",
    "assembler = VectorAssembler(inputCols = inputCols, outputCol = 'featuresCleanWithDatel1')\n",
    "dataset = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'featuresCleanWithDatel1', labelCol = 'fare_amount')\n",
    "model = lr.fit(dataset)\n",
    "summary = model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.66, 0.33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = assembler.transform(train)\n",
    "testDataset = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'featuresCleanWithDatel1', labelCol = 'fare_amount')\n",
    "model = lr.fit(trainDataset)\n",
    "summary = model.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
