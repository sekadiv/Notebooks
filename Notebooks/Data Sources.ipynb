{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL supports operating on a veriety of data sources through the DataFrame interface\n",
    "\n",
    "We'll exporle genral methods for loading and saving data suing the Spark Data Sources and then go into specific optins that are avalable for the build-in data sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genric load/save command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Datasources\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"salary_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+------+\n",
      "|Department|Designtion|FirstName|Salary|\n",
      "+----------+----------+---------+------+\n",
      "|     Admin|   Maneger|   Michel|  1000|\n",
      "|     Sales|    Junior|  Faraday|   900|\n",
      "| Marketing| Porfessor|   Newton|  1100|\n",
      "|     Sales| President|    James|  1000|\n",
      "|     Sales|   Maneger|     Bond|   700|\n",
      "|     Admin| President|   Donald|   900|\n",
      "| Marketing|    Junior|    Trump|   600|\n",
      "|     Admin|    Junior|    Rocco|   950|\n",
      "| Marketing| President|    Ethan|  1200|\n",
      "|     Admin|    Intern|     Hunt|   400|\n",
      "+----------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parquet is a columnar format that is supported by most of the big data processing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select (\"Department\",\"Designtion\",\"Salary\").write.save(\"anonymous_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Specifying Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"employees.json\", format = \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Borta|  4000|\n",
      "|  David|  4800|\n",
      "|  Sandy|  5100|\n",
      "|  Peter|  1500|\n",
      "|   John|  4700|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"profession.csv\", format = \"csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### options\n",
    "\n",
    "format to specify the file format \n",
    "\n",
    "inferScema to make spark figure out the data schema\n",
    "\n",
    "header tells spark that the file has column title row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-----------------+\n",
      "|  name|age|            title|\n",
      "+------+---+-----------------+\n",
      "|  john| 30|           doctor|\n",
      "|  Boby| 32|        Developer|\n",
      "| Peter| 29|        Scientist|\n",
      "| Kevin| 32|           Police|\n",
      "|Andrew| 37|      gym trainer|\n",
      "| Kumar| 40|software engineer|\n",
      "|   Tom| 51|             Chef|\n",
      "+------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save(\"random.parquet\", format = \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL queries directly be run on files \n",
    "\n",
    "instead of the table we can specify the table path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from parquet.`salary_data.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+------+\n",
      "|Department|Designtion|FirstName|Salary|\n",
      "+----------+----------+---------+------+\n",
      "|     Admin|   Maneger|   Michel|  1000|\n",
      "|     Sales|    Junior|  Faraday|   900|\n",
      "| Marketing| Porfessor|   Newton|  1100|\n",
      "|     Sales| President|    James|  1000|\n",
      "|     Sales|   Maneger|     Bond|   700|\n",
      "|     Admin| President|   Donald|   900|\n",
      "| Marketing|    Junior|    Trump|   600|\n",
      "|     Admin|    Junior|    Rocco|   950|\n",
      "| Marketing| President|    Ethan|  1200|\n",
      "|     Admin|    Intern|     Hunt|   400|\n",
      "+----------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's take a look at how the we load and save specific files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. json \n",
    "\n",
    "Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. \n",
    "\n",
    "This conversion can be done using SparkSession.read.json on a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"employees.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Borta|  4000|\n",
      "|  David|  4800|\n",
      "|  Sandy|  5100|\n",
      "|  Peter|  1500|\n",
      "|   John|  4700|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parquet files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"salary_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+------+\n",
      "|Department|Designtion|FirstName|Salary|\n",
      "+----------+----------+---------+------+\n",
      "|     Admin|   Maneger|   Michel|  1000|\n",
      "|     Sales|    Junior|  Faraday|   900|\n",
      "| Marketing| Porfessor|   Newton|  1100|\n",
      "|     Sales| President|    James|  1000|\n",
      "|     Sales|   Maneger|     Bond|   700|\n",
      "|     Admin| President|   Donald|   900|\n",
      "| Marketing|    Junior|    Trump|   600|\n",
      "|     Admin|    Junior|    Rocco|   950|\n",
      "| Marketing| President|    Ethan|  1200|\n",
      "|     Admin|    Intern|     Hunt|   400|\n",
      "+----------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  JDBC to other databases \n",
    "\n",
    "Spark sql can read data from other databases using JDBC. \n",
    "\n",
    "The output of this read operation is in the form of a dataframe. Which can easity be processed in Spark SQL or joined with other data sources.  \n",
    "\n",
    "Users can specify the JDBC connection properties in the data source options. user and password are normally provided as connection properties for logging into the data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above code is an example fo connecting with a external datbase. \n",
    "\n",
    "format  - tells that is database. \n",
    "\n",
    "url - specifies the address of the database\n",
    "\n",
    "dbtable - name of the table is to be used \n",
    "\n",
    "user - username to sign in to the database \n",
    "\n",
    "password - password for siging in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These are normally used options there are a lot of other options like this. We can expore them in the documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hive Tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqprk SQL also supports running Apache Hive query. \n",
    "\n",
    "Hive tabels have a lot of dependencies. Spark cannot infer these dependencies on its own. Having said that is we bring all these dependencies in the classpath of spark. Then Spark can fingure it out. \n",
    "\n",
    "For cluster applications these dependencies also need to be present on all the nodes. As these nodes will also need to seralise and deserealise the objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When working with HIVE we need to instanciate a spark session with HIVE support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_hive = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-93-166.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL Hive integration example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb1a919c750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### enableHiveSupport() \n",
    "\n",
    "This is the method which we need to call when bulding the SparkSession "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### After connecting HIVE\n",
    "\n",
    "Spark is connected to HIVE. Users can send queries in the same format as an SQL query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_hive.sql(\"CREATE TABLE IF NOT EXISTS company (company STRING, sales int) USING hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_hive.sql(\"LOAD DATA LOCAL INPATH 'sales_info.txt' INTO TABLE company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|     company|sales|\n",
      "+------------+-----+\n",
      "|   Google_Q1|  200|\n",
      "|   Google_Q2|  120|\n",
      "|   Google_Q3|  340|\n",
      "|   Google_Q4|  300|\n",
      "|Microsoft_Q2|  300|\n",
      "|Microsoft_Q3|  124|\n",
      "|Microsoft_Q4|  243|\n",
      "|Microsoft_Q1|  330|\n",
      "| Facebook_Q3|  870|\n",
      "| Facebook_Q2|  750|\n",
      "|    Apple_Q1|  650|\n",
      "|    Apple_Q2|  550|\n",
      "|    Apple_Q3|  750|\n",
      "|    Apple_Q4|  850|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_hive.sql(\"select * from company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hive = spark_hive.sql(\"select * from company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table partitioning \n",
    "\n",
    "This is a common optimisation approch used like Hive. In a partitioned table, data is usually stored in different directoried, with partitioning column values encoded in the path of each partition directiry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All built-in file sources (including Text/CSV/JSON/ORC/parquet) are able to discover and infer partitioning information automatically.\n",
    "\n",
    "example : date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
